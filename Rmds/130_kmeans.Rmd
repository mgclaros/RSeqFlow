### Background

[START EXPANDIBLE]: #
`r EXPAND_bx` Expand to read about <b><i>k</i>-means</b></summary>

_K_-means clustering is the most commonly used unsupervised machine learning algorithm for partitioning a given data set into a set of _k_ groups (i.e., _clusters_). Each cluster is represented by its center (i.e, _centroid_) which corresponds to the mean of points assigned to the cluster. The basic idea behind _k_-means clustering consists of defining clusters so that the total intra-cluster variation (known as total **within-cluster variation**) is minimised. There are several algorithms to do so, the the most widely used is the _Hartigan-Wong algorithm_ which defines the total within-cluster variation as the sum of squared distances Euclidean distances between items and the corresponding centroid.

_K_-means clustering is [very simple and fast algorithm](https://compgenomr.github.io/book/clustering-grouping-samples-based-on-their-similarity.html), making it appropriate for efficient dealing with very large sets of data. However, the final results are sensitive to the initial _centers_ and it is sensitive to outliers.

The [standard R function for _k_-means](https://www.statology.org/k-means-clustering-in-r/) clustering is `kmeans()` from the `stats` package, where you should define:

* ***centers***: Possible values are the number of clusters (_k_).
* ***iter.max***: The maximum number of iterations allowed. Default value is 10.
* ***nstart***: The number of random starting partitions when centers is a number. As the final result of _k_-means clustering result is sensitive to the random starting assignments, an `nstart = 25` is highly recommede, and even values up to `50` to have a more stable result.

As in HCA, the initial _k_ can be an arbitrary number or we can use objective metrics to obtain the better estimate. The `orientative_k` obtained [above](#orientative-plots) was determined by [WSS] for `kmeans()`. However, an adapted **gap statistic** can also be used, as well as the best-of-30 function `NbClust()`.

</details> 
[END EXPANDIBLE]: #


### Optimal clusters

Using the `m.ctf.std` matrix, the scree plot provided above the `orientative_k` for `kmeans()`. Let's now calculate another _k_s from **gap statistics**:

```{r}
# calculate gap statistic for each number of clusters
gap_stat <- clusGap(m.ctf.std, 
                    FUN = kmeans,    # change for hcut when k-means are analysed
                    nstart = 25, 
                    K.max = 10,   # the number of clusters evaluated
                    B = 50)       # the number of Monte Carlo (“bootstrap”) samples
gap_k <- ExtractFirstMax(gap_stat$Tab[,3])
message("Number of **k-means** clusters from **GAP statistics: ", gap_k, "**")

# plot of clusters vs. gap statistic
fviz_gap_stat(gap_stat) + labs(subtitle = "Gap statistic for k-means")
```

Let's now calculate the _k_ from the **best-of-30** methods:

```{r kmeans-of-30}
# bext-of-30 for kmeans
nb <- NbClust(m.ctf.std, 
              distance = "euclidean", 
              min.nc = 2,
              max.nc = 10, 
              index = "all",
              method = "kmeans")
# graphical representation
fviz_nbclust(nb)

# data from the barplot
table(nb$Best.nc[1,])

# extract k and groups
kmeans_groups_30 <- nb$Best.partition
kmeans_k30 <- as.integer(max(names(table(kmeans_groups_30))))
message("**Optimal** number of **k-means** clusters for genes analysing **30 methods** is: **", kmeans_k30, "**")

kable(t(as.data.frame(table(kmeans_groups_30))), caption = "K-means groups using the best-of-30 k")
```

The most objective value for _k_ is the one obtained by the best-of-30, and it will be retained.

```{r optKmeansk}
# kmeans_k <- min(orientative_k, gap_k, kmeans_k30)
kmeans_k <- kmeans_k30
message("The **final** number of **k-means** clusters is: **", kmeans_k, "**")
```



### Forming k groups

Let's exploit the `kmeans()` function to produce the clusters.

```{r kmeansCalc, fig.width=6, fig.height=6, out.width='50%', fig.show='hold', fig.align='center'}
# For CTF data
kmeans_ctf <- kmeans(m.ctf.std, 
                     centers = kmeans_k, 
                     nstart = 25)

groups_kmeans <- kmeans_ctf$cluster

# colours for groups
niceCols <- brewer.pal(kmeans_k, "Paired")

# pie chart of clusters
pie(table(groups_kmeans), 
    col = niceCols,
    labels = paste("Cluster", names(table(groups_kmeans))), 
    main = "k-means groups for CTF")
```


### Group details

In first instance, let's see the detailed number of genes per cluster and then how are they distributed in the two main dimensions.

```{r kmeansgenesingroups}
# see the number of genes per cluster
n_genes_group_kmeans <- table(groups_kmeans)
kable(t(as.data.frame(n_genes_group_kmeans)), caption = "Final K-means groups")
```

Since the distribution of genes depends on more than two dimensions (variables), the plot calculated with `fviz_cluster()` will perform principal component analysis (PCA) and plot the data points according to the first two principal components that explain the majority of the variance.

```{r kmeansVisu, fig.width=6, fig.height=6, fig.show='hold', warning=FALSE}
# see the distribution of genes per cluster
fviz_cluster(kmeans_ctf, 
             data = m.ctf.std, 
             main = "kmeans for CTF",
             # palette = niceCols, 
             palette = "Dark2",
             ggtheme = theme_minimal(),
             # ellipse.type = "norm",
             geom = "point")  # no gene label
```


```{r kmeansCentroids, eval=VERBOSE_MODE, results='asis'}
cat("The bidimensional plot of groups can be complemented with the observation of each cluster **centroid** and the distances of genes belonging to each cluster. \n")

fviz_cluster(kmeans_ctf, 
             data = m.ctf.std,
             palette = "Dark2", 
             ellipse.type = "euclid", # Concentration ellipse 
             star.plot = TRUE, # Add segments from centroids to items 
             repel = TRUE, # Avoid label overplotting (slow)
             ggtheme = theme_minimal(),
)
```

Since we know that the genes sould be distributed by the experimental factors, does the _k_-means clustering matches with the true structure of the data? 

We should compute a cross-tabulation between _k_-means clusters and the experimental factors

```{r TODO, echo=FALSE}
# table(iris$Species, km.res$cluster) PAGE 149

# table(m.ctf.std$ExpFactor, km.res$cluster)
```
