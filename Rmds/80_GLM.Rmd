### About GLMs {#aboutGLMs}

Here you can find why a generalised linear model ([GLM]) is the better approach to obtain differentially expressed genes ([DEG]s), and why the `limma-voom-treat` is the most appropriate approach.

[START EXPANDIBLE]: #
`r EXPAND_bx` Expand to read about the <b>use of [GLM]s in RNA-seq</b></summary>

Remember from [above](#differential-expression) that linear regression (performed with the `stats::lm()` funcion) is a useful statistical method we can use to understand the relationship between two variables, provided that [the following assumptions are met for linear models](https://www.geeksforgeeks.org/assumptions-of-linear-regression/):

1. **Linear relationship** between the independent variable _x_ and the dependent variable _y_.
2. **Homoscedasticity** of residuals in linear regression, that is, the residuals have constant variance at every level of _x_.
3. **Normality** in the distribution of the residuals of the model.
4. **Independence of errors** in the residuals; in particular, there is no correlation between consecutive residuals in time series data.
5. **Lack of multicollinearity**, that is, when several independent variables are used, they are not highly correlated between them.

Since RNA-seq data usually have heterogeneous sample sizes and the results are counts, **they violate at least the normality requirements** of [linear regression](#differential-expression) because they follow a **negative binomial ([NB]) distribution**, leading to ill-behaved _P_-values and likely failed [FDR] control [@BenjaminiHochberg1995ps]. 

It is also common that **RNA-seq data present heteroscedasticity**, that is, absence of variance homogeneity, as will be shown below. This is why this deviation must be corrected.

</details> 
[END EXPANDIBLE]: #


[START EXPANDIBLE]: #
`r EXPAND_bx` Expand to read about the <b>[GLM] algorithms for differential expression</b></summary>

In RNA-seq, [DE] parametric tests require methods based on [NB] models that take advantage of the estimates of dispersion and logarithmic fold changes, their main issue being the false positive. This issue has been recently investigated [@Li2022cf] for the most popular methods based on parametric distributional assumptions [DESeq2] and [edgeR], as well as `[limma]-voom`, `NOISeq`, `dearseq` and Wilcoxon rank-sum test. **The only method that maintains the [FDR] control is the Wilcoxon rank-sum test**, while [DESeq2] and [edgeR] sometimes exceed 20% of false positives, a rate that increases with the number of samples since they are very affected by the existence of outliers. In extreme cases, [DESeq2] and [edgeR] had only an 8% overlap in the DEGs they identified. 

`r INFER_bx`
The recommendation of [@Li2022cf] is to **use non-parametric Wilcoxon rank-sum test with large sample sizes**, with _large_ meaning $>10$ samples per experimental condition, since it is **more robust to outliers**. The Wilcoxon test developed in 1956 [@WILCOXON1946wa] is also known as the Mann-Whitney test, developed in 1947 [@Mann-Withney1947]. Wilcoxon rank-sum test is known to be **powerful for skewed distributions**, as is the case with gene expression counts measured by RNA-seq.
</div>

Non-parametric approaches, such as `NOISeq` `SAMseq` or the Wilcoxon test require more replicates (usually 5 or more) than parametric methods (usually 3, but some of them can work with only 1 replicate [@GonzalezGayte2017]). The most frequent case when studying non-model species is that the number of replicates per experimental condition is not so high (usually 3 or less than 5) that allows a non-parametric study. Therefore, the parametric analysis that provides the best results in the recent study of @Li2022cf is the **combination of `limma + voom`** (very similar to the `limma-trend`, but more powerful). This is a [GLM] approach that extends classical [linear models](#differential-expression) to [non-normally distributed response data](https://online.stat.psu.edu/stat504/lesson/6/6.1) (including linear regression, logistic regression and Poisson regression) [@Nelder1972GLM], where `glm()` is used to fit [GLM]s on the variable whose heteroskedascity has been corrected. 

Obtaining [DEG]s using [edgeR] and [limma] libraries can be addressed by three different parametric approaches:

- **Exact _t_-test** is based on the Student's _t_-test [@Robinson2008ro] for a difference in mean between two groups of negative binomial random variables. It depends on [`exactTest()` function](https://www.r-bloggers.com/2020/09/exact-tests-and-plots-with-edger-basic-differential-expression-analysis/). It is rapid and easy-to-do, but **many false positives** are obtained, as revealed by [@Li2022cf].
- **Quasi-likelihood F-tests (QLF)** performs the quasi-likelihood (QL) F-test [@Lund2012; @Lun2016] based on [GLM], with some enhancements and slight differences on trend and [FDR] methods. In fact, it performs a _genewise negative binomial generalized linear model with quasi-likelihood tests_ with `glmQLFit()` (preferred over `glmLRT()`) and `glmQLFTest()` (or the more rigorous `glmTreat()`) functions. Since they provide the most accurate type I error control and adds a bayesian squeezing of data from [limma] package, the results are better than exact tests for RNA-seq under the assumption that genes should exhibit a [binomial negative][NB] distribution, as [indicated above](#binomialNegative).
- A pure **generalized linear model (GLM)** based on `lmFit()` and `eBayes()`, or even better `treat()`, that gives the **lowest number of false positives** [@Law2014tt; @Li2022cf]. The [GLM] is thought to deal with heterogeneity in variance (heteroskedascity) better than pure linear models.

</details> 
[END EXPANDIBLE]: #


[START EXPANDIBLE]: #
`r EXPAND_bx` Expand to read about the <b>empirical Bayes moderation</b> in RNA-seq</summary>

An indicated above, both exact _t_-test and [QLF] produce an unwanted number of false positives, while `limma + voom` [GLM] provides more reliable results by means of the following functions:

* `lmFit()`, that fits a linear model by weighted or generalized least squares for each gene given a series of data matrices corresponding each one to a different sample.
* `contrasts.fit()`, that computes estimated coefficients and standard errors for a given set of contrasts for a linear model fit using `lmFit()`. The idea of this function is to fit a full-rank model to obtain coefficients and standard errors for any number of contrasts.
* `eBayes()` for the fitted model, computing moderated _t_-statistics, moderated _F_-statistic, and log-odds of differential expression by empirical Bayes moderation. This is to compensate the few replicates usually available in RNA-seq experiments, as explained [above](#aboutGLMs) and in the background about [data dispersion](#binomialNegative).

But this approach is highly dependent on _P_ values, that are misused in many cases.

</details> 
[END EXPANDIBLE]: #



[START EXPANDIBLE]: #
`r EXPAND_bx` Expand to read about <b>the usual misinterpretation of <i>P</i>-values</b></summary>

Statistical significance offers the benefit of simplicity and clarity on the one hand. 
But the size of the effect is not indicated by the statistical significance.
Those and others drawbacks of the _P_-value led some to advocate for more sophisticated models (such as the Bayesian approach mentioned above). One of those agents claiming the proper use and interpretation of the _P_-value is The American Statistical Association (ASA), which highlights that **scientific conclusions should not be based only on whether a _P_-value passes a specific threshold since, by itself, a _P_-value does not provide a good measure of evidence regarding a model or hypothesis**. 

Since associating statistically significant findings with $P < 0.05$ (as arbitrarily introduced by Ronald Fisher in 1925) results in a high rate of false positives, the ASA also recommends that **declarations realted to _statistical significance_ should be abandoned** since statistical significance was never meant to imply scientific importance [@Wasserstein2019] and it has promoted conscious or unconscious bad practices in research [@Mayo2022aa]. The situation is so worrying that some authors claim that most published research findings are false [@Ioannidis2005ud] or highly biased [@Altman2017mv]. 

**Common misinterpretation of statistical significance**:

The following limitations have been described in detail in [@Amrhein2017aa,@Amrhein2019,@Montero2023aa].

* **$P$-values**: they ~~do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone~~, but the probability of the test statistic assuming the null hypothesis. In fact, _P-values depend on your data and are not reliable_.
* **Confidence intervals**: many researchers incorrectly believe that ~~the confidence interval from an isolated experiment has a 95% chance of containing the true value, or intervals that will contain the parameter with a given probability~~. They simply are a frequency description of samples from a population in large-scale resampling of this population. Be aware that _many researchers wrongly interpret confidence intervals as Bayesian credible intervals_.
* **Statistical significance**: it was introduced as a tool to suggest interesting results and perform further confirmatory research and not as ~~a true-difference/no-difference decision boundary~~. Subsequently, the variables selected as being truly changed are considered to be scientifically significant and merit more research. _The absence of statistical significance does not indicate an absence of an effect_. And the _presence of statistical significance does not mean scientific significance_.
* **False discovery rate ([FDR])**, also known as the _type I error rate_, is usually set to control the ratio of false positives when multiple hypothesis tests are performed, such as in omics studies. But this approach requires independent variables, which is not the case, since many genes/proteins are acting on other genes/proteins, and _[FDR] test statistic cannot be used since it would drive and potentially bias the extraction of the biological information contained in the omic data_.


In fact, in omics sciences, the frequent misuse of _P_-values is limiting the generation of robust scientific knowledge [@Montero2023aa,@Amrhein2019]:

* biological inferences are derived only from biomolecules with statistically significant changes;
* only the biomolecules with statistically significant changes are selectively reported (biasing the published results); and
* asterisks or inequalities ($P < 0.05, P < 0.001$) are reported instead of the obtained _P_-values. 

To circumvent these misinterpretations:

* You should never conclude that there is _no difference_ or _no association_ just because $P < 0.05$ because a confidence interval includes zero or your reference in the H0 hypothesis. 
* You should never conclude that two studies conflict because one had a statistically significant result and the other did not.
* _P_-values should be given with a precise value ($P = 0.021$), without adornments such as stars or asterisks, and not as binary inequalities (~~$P < 0.05$~~)
* You must focus on the parameter that measures your biological effect (for example, the fold-change) and not the _P_-values or the confidence intervals.

![Beware false conclusions](https://media.nature.com/lw800/magazine-assets/d41586-019-00857-9/d41586-019-00857-9_16551622.jpg)



**Possible solutions**: 

It is obvious that statistics naturally vary from study to study and can lead to large disparities in _P_-values, far beyond falling just to either side of the 0.05 threshold (see the image above). Therefore, **we must consider uncertainity** and, for example, re-think _confidence intervals_ as **compatibility intervals** to transmit the idea that [all the values between the interval's limits are reasonably compatible with the data](https://github.com/matloff/regtools/blob/master/inst/NoPVals.md). Hence, because the interval gives the values most compatible with the data, it doesn't mean values outside it are incompatible: they are just less compatible. Therefore, **compatibility intervals are more informative and more intuitive**. 


Second, it is not a question of abandoning the use of statistical significance since it would favour unjustified claims and mislead science, favouring non-ethical practices [@Wasserstein2019]. However, a better use of hypothesis tests and _P_-values by researchers in the new context of _compatibility intervals_ is necessary:

1. Change to lower _P_-values (for example $P < 0.005$) to reduce the false positives and improve the reproducibility of research [@Benjamin2018sj]. It is simple to implement but would produce an enormeous amount of false negative results that favours the new perception that [low _P_ values does imply more significance or more importance](https://github.com/matloff/regtools/blob/master/inst/NoPVals.md). Additionally, more stringent _P_values will require larger samples (that may not be feasible).
2. Avoid _dichotomisation_ (significant vs. non-significant) as much as possible [@Amrhein2019] and learn to deal with uncertainty.
3. Contrary as is commonly thought, ~~do not select genes by the _P_-value and then use the fold-change for priorisation~~, but select them by the fold-change (that focuses on the size of effect you are analysing) and then priorise by the _P_-value only if some filtering is required [@Mayo2022aa,@Montero2023aa]. Then, repeat the experiment to confirm your results, even though you obtain different (but compatible) fold-changes and _P_-values.
4. Add a Bayesian perspective to the analysis to produce _credible intervals_ corresponding to the distributions of the plausibility of the values of the parameter. Its main drawback is that when the number of samples is low, result may be hihgly biased.


However, it is recognised that _P_-values and statistical significance are a prerequisite to proof that the outcomes are not random. But for understanding the effect magnitude, at least a second parameter should be considered, such as the fold-change in genes, proteins or metabolites: 

1. _P_-values must be integrated with secondary results to arrive at valid conclusions [@Lu2015wd]. In the case of RNA-seq, the secondary result is the **expression fold-change**, that reflects the change magnitude.
2. Researchers should seek to analyse data in multiple ways (including statistics) to see whether different analyses converge on the same answer [@Nature2019].
3. Avoid selecting genes by statistical significance relying on solid cutoffs. Instead, relax cutoffs and interpret your outcomes in the context of previous knowledge and the known relationships among genes and proteins [@Betensky2019].

</details> 
[END EXPANDIBLE]: #



[START EXPANDIBLE]: #
`r EXPAND_bx` Expand to read about <b>using `treat()` to circumvent <i>P</i>-value issues`</b></summary>

In the context of avoiding fixed _P_-values and the use of expression fold-change as the second endopoint for differential expression, [edgeRUsersGuide] proposed to use `treat()`, that tests whether the true log2-fold-change is greater than `FC` in absolute value [@McCarthy2009oo]. As a result, `treat()` is a more rigorous and conservative alternative to `eBayes()`. This `treat()`-based approach is also known as the **limma-trend method** [@Law2014tt] in which, instead of testing for genes that have true log-fold-changes different from zero, it is tested whether the true log2-fold-change is greater than the used defined `FC` and `P` values [@Law2016yj].

When the number of DEGs is large, `treat()` is often useful for giving preference to larger `FC`  and for **prioritising genes that are biologically important**. Note that the `FC` threshold is not the minimum value of the fold-change expected to qualify a [DEG]. [DEG]s will need to exceed this threshold by some way before being declared statistically significant. It is better to interpret the `FC` threshold as _the fold-change below which we are definitely not interested in the gene_ rather than ~~the fold-change above which we are interested in the gene~~.

`r NOTE_bx`
`FC` threshold for `treat()` must be relatively small because genes need to have FCs substantially greater than defined threshold. Typical threshold is $FC = 1.2$ (or even 1.1 or 1.5) to the purpose of prioritising genes with larger fold-changes. Therefore:

* $FC = 2$ is considered **high** and counter productive.
* When $FC = 1$, `treat()` is identical to `eBayes()`.
</div>

In the presence of a huge number of DEGs, a relatively large `FC` threshold may be appropriate to narrow down the search to genes of interest. In the lack of DEGs a small or even no fold-change threshold shall be used.

Note that the cutting threshold of `FC` will vary depending on the counts and more statistics. Use `topTreat()` to examine genes analysed using `treat()`. 

</details> 
[END EXPANDIBLE]: #

`r INFER_bx`
The above discussion reflects that changes in thresholds for `FC` and `P`, the statistic approach and even the RNA integrity affects the differential expression of any transcribed RNA (messanger, non-coding, etc.), and may exhibit significant differences [@Lu2022md; @Spies2019mx]. Fortunately, the functional enrichment analysis of the [DEG]s was significantly consistent and identified regular characteristics of samples. Hence, functions instead of genes must be the main source of interpretations when the statistic approach is not the best performing one [@Lu2022md; @Spies2019mx].
</div>

### Advantages of limma-voom-treat

Benefits of the [limma] library using the `voom()` function combined with `treat()` function (instead of `eBayes()`) are the following:

1. production of accurate results for RNA-seq [@Law2014tt; @Li2022cf];
2. resilience for unequal library sizes [@Law2014tt];
3. dealing with outliers better than other parametric approaches [@Soneson2013];
4. exacerbation of experimental errors when normalisation is based on less suitable adjustments, such as RPKM, CPM or TMP instead of [TMM].


`r OK_bx`
Taking all the above together explains why this `r paste(SOFT_NAME, VERSION_CODE)` script is based only on `limma + voom + treat` [GLM].
</div>



### Fitting the GLM

Homoskedastic GLM-fitted values will be analysed with `eBayes()` to produce `xxxx.eB` variables, and with `treat()` to produce `xxxx.treat` variables.

```{r GLM-fit}
v.tmm.fit <- lmFit(v.tmm, design)
v.tmm.fit <- contrasts.fit(v.tmm.fit, contrasts = contr_matrix)
# using the classical eBayes()
v.tmm.fit.eB <- eBayes(v.tmm.fit)
# using the adjusted treat()
v.tmm.fit.treat <- treat(v.tmm.fit, lfc = logFC)

# remove needless variables
rm(v.tmm.fit)
```


### Resulting DEGs

Let's have an overview of DEGs per contrast depending on the `eBayes()` or `treat()` analysis based on the `decideTests()` function with the configured `P` and `FC` thresholds, [as recommended above](#aboutGLMs), that qualifies genes in each contrast as:

* **NotSig** (`0`): not-significant expression change
* **Down** (`-1`): genes <span style="color:blue">down-regulated</span> in the contrast
* **Up** (`+1`): genes <span style="color:red">up-regulated</span> in the contrast


Although using the default `method = separate` makes function is equivalent to `topTable()` (that will be used for saving [DEG]s) but less stringent. Since `treat()` requires lower `P` and `FC` thresholds than `eBayes()`, the default `P` and `FC` will be increased to `0.05` and `1.5` for the Bayesian approach.

```{r DEGs-ebayes-treat}
# reset P and logFC thresholds for eBayes
P_eB_MIN <- 0.05
P_eB <- ifelse(P > P_eB_MIN, P_eB_MIN, P)
logFC_eB_MIN <- log2(1.5)
logFC_eB <- ifelse(logFC < logFC_eB_MIN, logFC_eB_MIN, logFC)

# NotSig, Down and Up variables from decideTests() will be used to colour plots
status.eB <- decideTests(v.tmm.fit.eB, p.value = P_eB, lfc = logFC_eB)
status.treat <- decideTests(v.tmm.fit.treat, p.value = P, lfc = logFC)

# join per rows the summary tables of eBayes and treat, with an intermediate title 
tmp <- rbind(EBAYES = "", 
             summary(status.eB), 
             TREAT = "", 
             summary(status.treat))
# display the summary table
kable(as.data.frame(tmp), 
      align = "r", 
      caption = "Orientative number of DEGs per contrast depending on eBayes() and treat() functions")
```

```{r old-DEGs-ebayes-treat, include=FALSE, eval=FALSE}
# dt.norm2 <- decideTests(v.tmm.fit.eB, p.value = P, lfc = logFC)
# this is now status.eB
t1 <- as.data.frame(summary(status.eB))
t2 <- as.data.frame(summary(status.treat))
# construye un dataframe con los resultados con P, FC por eBayes y treat
t3 <- cbind(t1, t2[,3])
colnames(t3) <- c("Expression", "Contrast", "eBayes()", "treat()")

# display the summary table
kable(t3, align ="lcrr")
```


### MD plots {.tabset .tabset-fade .tabset-pills}

A mean-difference plot ([MD plot]) is a representation of log2-intensity ratios (differences) versus log2-intensity averages (means). It is equivalent to [MA plot](https://en.wikipedia.org/wiki/MA_plot) from microarrays. Thresholds for `P` and `FC` lined to clearly see <span style="color:blue">down-regulated</span> genes in <span style="color:blue">**blue**</span>, while <span style="color:red">up-regulated</span> genes are in <span style="color:red">**red**</span>.

#### Global plot

Let's see the global aspect of DEGs.

```{r MDplotx3, fig.width=4.5, fig.height=5, out.width=c('50%', '50%'), fig.show='hold'}
plotMD(v.tmm.fit.eB, 
       status = status.eB, 
       hl.cex=0.5,
       main = "DEGs by eBayes()")
# mark logFC cutoffs
abline(h = c(-logFC_eB, logFC_eB), col = "#7FFFD4")

plotMD(v.tmm.fit.treat, 
       status = status.treat, 
       hl.cex=0.5,
       main = "DEGs by treat()")
# mark logFC cutoffs
abline(h = c(-logFC, logFC), col = "magenta")
abline(h = c(-logFC_eB, logFC_eB), col = "#7FFFD4", lty = 2)
```

`r NOTE_bx`
Since [FC] and _P_ thresholds are different, [DEG]s obtained with `treat()` should not be a subset of those obtained by `eBayes()`.
</div>

```{r Compare-DEGs-tr_eB, fig.width=5, fig.height=5, out.width='55%', fig.align='center'}
# create vector of gene IDs to compare
degs.ebayes <- which(rowSums(abs(status.eB)) > 0)
degs.treat <- which(rowSums(abs(status.treat)) > 0)
# gather vectors in a single list with *named* elements
venn.list <- list(DEGs_eBayes = degs.ebayes, 
                    DEGs_treat = degs.treat)

## construct a nice Venn diagram
gv <- ggvenn(venn.list,
       fill_color = VENN_COLORS[1:length(venn.list)],
       set_name_color = VENN_COLORS[1:length(venn.list)],
       stroke_size = 0.5,
       set_name_size = 4,
       text_size = 4,
       auto_scale = FALSE) +
    ggtitle("Total DEGs for eBayes() and treat()\n") 
gv

# venn.matrix <- venn(venn.list,
#                     show.plot = TRUE,
#                     intersections = TRUE)
# title("Total DEGs for eBayes() and treat()")

# remove needless variables
rm(degs.ebayes, degs.treat, venn.list)
```

#### Plot every contrast

Now let's see the differences of DEGs depending on the `eBayes()` and `treat()` methods.

```{r DEGsPerContrast, fig.width=4.5, fig.height=5, out.width=c('50%', '50%'), fig.show='hold'}
# a variable for the number of contrasts to study.
NUM_CONTRASTS <- length(allContrasts)
i <- 1
for (i in 1:NUM_CONTRASTS) {
  # plot for eBayes
  plotMD(v.tmm.fit.eB, 
         column = i,
         status = status.eB[ ,i], 
         main = paste0("eBayes() - ", colnames(v.tmm.fit.eB)[i]),
         xlim = c(-8, 12), 
         ylim = c(-10, 10),
         hl.cex=0.5)
  # mark logFC cutoffs
  abline(h = c(-logFC_eB, logFC_eB), col = "#7FFFD4")

  # plot for treat
  plotMD(v.tmm.fit.treat, 
         column = i,
         status = status.treat[ ,i], 
         main = paste0("treat() - ", colnames(v.tmm.fit.treat)[i]),
         xlim = c(-8, 15), 
         ylim = c(-10, 10),
         hl.cex=0.5)
  # mark logFC cutoffs
  abline(h = c(-logFC, logFC), col = "magenta")
  abline(h = c(-logFC_eB, logFC_eB), col = "#7FFFD4", lty = 2)
}
```
